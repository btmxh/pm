\chapter{Notations} % (fold)
\label{chap:Notations}

\section{Einstein summation convention} % (fold)
\label{sec:Einstein summation convention}

Let \( V \) be a vector space (which will be called a \textbf{linear space} from
now on). Then, consider a basis \( B \), which is a collection of basis vectors
\( b_{1}, b_{2}, b_{3}, \ldots, b_n \), and a vector \( v \in V \). Because of
the basic property of basis, there exists unique \( \lambda_{1}, \lambda_{2},
\lambda_{3}, \ldots, \lambda_n \) such that:
\[
  v = \sum_{i=1}^{n} \lambda_i b_i
.\] 

\textbf{Einstein summation convention} (ESC) helps with reducing clutter in
this formula. First, we remove the summation sign, leaving us with only \(
v=\lambda _{i} b _{i} \).

However, if one changes the left hand side of the expression to \( v_{i} \), the
expression will have a different meaning, as the index \( i \) will no longer be
iterated over and there will no longer be any summation. Therefore, if one want
to denote an expression such as \( a_{1}+a_{2}+a_{3}=b_{1}+b_{2}+b_{3} \), one
either use a dummy variable \( s = a_{1}+a_{2}+a_{3}=b_{1}+b_{2}+b_{3} \), or
simply resort back to the sigma summation convention: \( \sum_{n=1}^{3}
a_{n}=\sum_{n=1}^{3} b_{n} \).

Going back to ESC, consider the problem of matrix multiplication, which is
generally written using normal notation as
\[
  (AB)_{ij} = \sum_{k=1}^{K} A_{ik}B_{kj}
.\] , which could be even further shortened as \( (AB)_{ij} = A_{ik}B_{kj} \).
With this notation, one would not have to pay attention to the range of \( k \),
which could be implicitly inferred from the dimensions of the matrices. This
expression also featured a glaring issue with normal convention: the ambiguity
between the \( ik \)-th coordinate of a vector and the entry at row \( i \) and
column \( k \) of a matrix. There are also notations that consider \( A_{i} \)
to be the \( i \)-th column of a matrix \( A \), so one could not just rely on
the type of \( A \) to distinguish between the two cases.

A solution to this prolem is to denote the row and column index at different
positions: \( A^{i}_{j}    \) as the entry at row index \( i \) and column index
\( j \). This notation not only resolve the ambiguity mentioned above, but it
also give us free notations for extracting a row and a column from a matrix: \(
A^{i} \) as the \( i \)-th row of \( A \), as a row vector, and \( A_{j} \) as
the \( j \)-th column of \( A \), as a column vector.

Now, rewrite the matrix vector multiplication formula using the new notation, we
have \( (AB)^{i}_{j} = A^{i}_{k}B^{k}_{j}  \). We can even drop the index \( k
\) as \( A^{i}_{k}B^{k}_{j}  \) is the product between the row vector \( A^{i}
\) and the column vector \( B_{j}\), leaving us with \( (AB)^{i}_{j} = A^{i}
B_{j}  \).

These subscripts and superscripts could be thought of as row and column
extracting operators: \( ()^{i} \) and \( ()_{j} \). We will take a closer look
at these operations, in order to derive several interesting properties which
will be very helpful in manipulating matrix identities.

\begin{theorem}
  Let \( A \) be an arbitrarily-sized matrix. Then \( (A^{i})^{T} = (A^{T}
  )_{i} \) (with \( A^{T}  \) denoting the \textit{transpose} of \( A \)).
\end{theorem}

\begin{proof}
  Denote \( E(i, j) \) as the matrix with all zeros, except only one \( 1 \)
  entry at row \( i \) and column \( j \). Then, one can write \(
  A=A^{i}_{j}E(i, j) \), and this decomposition is unique for all matrices
  \( A \).

  The left hand side (LHS) of the expression is \( (A^{i})^{T} = ((A^{k}_{j}E(k,
  j))^{i})^{T} = (A^{i}_{j}E(i, j))^{T} = A^{i}_{j}E(j, i)   \).

  The right hand side (RHS) of the expression is \( (A^{T}
  )_{i}=((A^{k}_{j}E(k, j))^{T} )_{i} = (A^{k}_{j}E(j, k))_{i} = A^{i}_{j}E(j,
  i) \).

  Hence, we have \( (A^{i})^{T} = (A^{T} )_{i}  \).
\end{proof}

Letting \( B=A^{T}  \), we have an equivalent identity: \( (B_{i})^{T} =
(B^{T})_{i} \).

\begin{quote}
  The theorem is very intuitive and can be proved using words. The \( i \)-th
  row of a matrix, after the matrix is transposed, became the \( i \)-th column
  of the transposed matrix.

  \[
    A = \begin{bmatrix}
      1 & 2 & 3 \\
      4 & 5 & 6 \\
      7 & 8 & 9
      \end{bmatrix} \implies A^{T} = \begin{bmatrix}
      1 & 4 & 7 \\
      2 & 5 & 8 \\
      3 & 6 & 9
    \end{bmatrix} \\
  .\]
  \[
    A_{2} = \begin{bmatrix} 2 \\ 5 \\ 8 \end{bmatrix} \implies
    (A^{T})^{2} = \begin{bmatrix} 2 & 5 & 8 \end{bmatrix} 
  .\] 

  Finally, another transpose needs to be done, in order to convert from a row
  vector to a column vector.
\end{quote}

Using the result of this theorem, one can prove the following theorem, which is
very powerful in manipulating matrix identities:

\begin{theorem}
  Let \( A_{1}, A_{2}, A_{3}, \ldots A_{n} \) be arbitrary matrices such that
  the product \( A_{1}A_{2}\ldots A_{n}  \) makes sense. Then, one has the
  following identities:

  \begin{itemize}
    \item \( (A_{1}A_{2}\ldots A_{n})^{i} = A_{1}^{i}A_{2}\ldots A_{n} \)
    \item \( (A_{1}A_{2}\ldots A_{n})_{j} = A_{1}A_{2}\ldots (A_{n})_{j} \)
  \end{itemize}
\end{theorem}

\begin{proof}
  Assuming the first identity is true, one can trivially derive the second
  identity as such:
  \begin{align*}
    B = A_{1}A_{2}\ldots A_{n} \implies (B_{j})^{T} = (B^{T})^{j} &=
    (A_{n}^{T}\ldots A_{2}^{T}A_{1}^{T})^{j} \\
                                                                  &=
                                                                  (A^{T})^{j}\ldots
A_{2}^{T}A_{1}^{T}
  .\end{align*}

  Transposing both sides yields the second identity.

  Returning to the first identity. Denote \( A = A_{1} \) and \( B = A_{2}A_{3}
  \ldots A_{n} \),
  then the identity is reduces to the \( n = 2 \) case: \( (AB)^{i} = A^{i}B \)
  .

  Expanding \( AB \) using ESC, we have \( (AB)^{i}_{j} = A^{i}B_{j} \). Then,
  one can "trim" the subscript \( j \) away by using the standard basis, which
  is very trivial and will not be discussed here.
\end{proof}

These operations are also linear, which is trivial to prove.

To conclude, the row and column extracting operations are linear operators on
matrices (and therefore row vectors, column vectors and scalars). The
column extraction of a product of matrices is the same matrix product, with the
last term replaced by its corresponding column extraction. Similarly, the row
extraction of a product of matrices is that same product, with the first term
replaced with its row extraction.

% section Einstein summation convention (end)

% chapter Notations (end)
